{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('/content/police-records-project'):\n",
    "    !git clone https://github.com/c-goenka/police-records-project.git\n",
    "    %cd /content/police-records-project\n",
    "    !pip install -r requirements.txt\n",
    "else:\n",
    "    %cd /content/police-records-project\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/content/drive/MyDrive/police-records-project-data/processed\"\n",
    "\n",
    "setfit_df = pd.read_csv(f\"{data_dir}/setfit_results.csv\")\n",
    "embeddings_df = pd.read_csv(f\"{data_dir}/embeddings_classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    y_true = df['true_label']\n",
    "    y_pred = df['pred_label']\n",
    "\n",
    "    return {\n",
    "        'Macro F1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'Accuracy': (y_true == y_pred).mean(),\n",
    "        'Correct/Total': f\"{(y_true == y_pred).sum()}/{len(y_true)}\"\n",
    "    }\n",
    "\n",
    "setfit_metrics = compute_metrics(setfit_df)\n",
    "embeddings_metrics = compute_metrics(embeddings_df)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'SetFit': setfit_metrics,\n",
    "    'Embeddings+Classifier': embeddings_metrics\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "setfit_f1 = comparison_df.loc['Macro F1', 'SetFit']\n",
    "embeddings_f1 = comparison_df.loc['Macro F1', 'Embeddings+Classifier']\n",
    "\n",
    "if setfit_f1 > embeddings_f1:\n",
    "    winner = \"SetFit\"\n",
    "    winner_f1 = setfit_f1\n",
    "    diff = setfit_f1 - embeddings_f1\n",
    "else:\n",
    "    winner = \"Embeddings+Classifier\"\n",
    "    winner_f1 = embeddings_f1\n",
    "    diff = embeddings_f1 - setfit_f1\n",
    "\n",
    "print(f\"\\nBest Method: {winner}\")\n",
    "print(f\"Macro F1: {winner_f1:.4f}\")\n",
    "print(f\"Margin: {diff:.4f}\\n\")\n",
    "print(f\"Dataset: 75 train / 23 test samples across 11 classes\")\n",
    "print(f\"Both methods use class balancing to handle imbalanced data\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
